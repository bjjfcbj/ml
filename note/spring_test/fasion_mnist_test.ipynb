{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('../../data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('../../data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.90)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 1000,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1,  1000] ava_loss: 1.76959\n[1,  2000] ava_loss: 0.78833\n[1,  3000] ava_loss: 0.71147\n[1,  4000] ava_loss: 0.65232\n[1,  5000] ava_loss: 0.59532\n[1,  6000] ava_loss: 0.56110\n[1,  7000] ava_loss: 0.52627\n[1,  8000] ava_loss: 0.52778\n[1,  9000] ava_loss: 0.50799\n[1, 10000] ava_loss: 0.48979\n[1, 11000] ava_loss: 0.44199\n[1, 12000] ava_loss: 0.44952\n[1, 13000] ava_loss: 0.44371\n[1, 14000] ava_loss: 0.40988\n[1, 15000] ava_loss: 0.41333\n[2,  1000] ava_loss: 0.40849\n[2,  2000] ava_loss: 0.40296\n[2,  3000] ava_loss: 0.38900\n[2,  4000] ava_loss: 0.37342\n[2,  5000] ava_loss: 0.38451\n[2,  6000] ava_loss: 0.36791\n[2,  7000] ava_loss: 0.35554\n[2,  8000] ava_loss: 0.35122\n[2,  9000] ava_loss: 0.36175\n[2, 10000] ava_loss: 0.35990\n[2, 11000] ava_loss: 0.36147\n[2, 12000] ava_loss: 0.35014\n[2, 13000] ava_loss: 0.34131\n[2, 14000] ava_loss: 0.36538\n[2, 15000] ava_loss: 0.31961\n[3,  1000] ava_loss: 0.34262\n[3,  2000] ava_loss: 0.31947\n[3,  3000] ava_loss: 0.31791\n[3,  4000] ava_loss: 0.33345\n[3,  5000] ava_loss: 0.30734\n[3,  6000] ava_loss: 0.33451\n[3,  7000] ava_loss: 0.31671\n[3,  8000] ava_loss: 0.32457\n[3,  9000] ava_loss: 0.31434\n[3, 10000] ava_loss: 0.31871\n[3, 11000] ava_loss: 0.32165\n[3, 12000] ava_loss: 0.32502\n[3, 13000] ava_loss: 0.31872\n[3, 14000] ava_loss: 0.30404\n[3, 15000] ava_loss: 0.29910\nFinished Training\n"
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "            scheduler.step()\n",
    "            print(\"[%d,%6d] ava_loss: %.5f\"%(epoch+1,i+1,running_loss/1000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"../../pth/fasion_mnist_test.pth\"\n",
    "torch.save(net.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy of the network: 87 %\n"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy of T-shirt/top : 76 %\nAccuracy of Trouser : 96 %\nAccuracy of Pullover : 79 %\nAccuracy of Dress : 88 %\nAccuracy of  Coat : 87 %\nAccuracy of Sandal : 91 %\nAccuracy of Shirt : 68 %\nAccuracy of Sneaker : 95 %\nAccuracy of   Bag : 96 %\nAccuracy of Ankle Boot : 96 %\n"
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ]
}